import streamlit as st
import pandas as pd
import os
from main import Data_Mining

CSV_PATH = r"C:\Users\Elyar\Desktop\linkdin-job-information\job_data.csv"

st.set_page_config(page_title="LinkedIn Job Scraper", layout="wide")
st.title("ğŸ” Ø¬Ø³ØªØ¬ÙˆÛŒ Ø²Ù†Ø¯Ù‡ Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ÛŒ Ø´ØºÙ„ÛŒ LinkedIn")

with st.form("job_form"):
    job_title = st.text_input("ğŸ”¤ Ø¹Ù†ÙˆØ§Ù† Ø´ØºÙ„", value="java")
    location = st.text_input("ğŸ“ Ù…Ú©Ø§Ù†", value="germany")
    count = st.number_input("ğŸ“¦ ØªØ¹Ø¯Ø§Ø¯ Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬", min_value=1, value=50)
    submitted = st.form_submit_button("ğŸš€ Ø´Ø±ÙˆØ¹ Ø¬Ø³ØªØ¬Ùˆ Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬")

if submitted:
    scraper = Data_Mining()
    try:
        with st.spinner("ğŸ§­ Ø¯Ø± Ø­Ø§Ù„ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø±ÙˆØ±Ú¯Ø±..."):
            scraper.setup_driver()

        with st.expander("ğŸ” Ù…Ø±Ø­Ù„Ù‡ ÙˆØ±ÙˆØ¯ Ø¨Ù‡ LinkedIn", expanded=True):
            st.markdown("âœ… Ù„Ø·ÙØ§Ù‹ Ø¯Ø± Ù…Ø±ÙˆØ±Ú¯Ø± Ø¨Ø§Ø²Ø´Ø¯Ù‡ ÙˆØ§Ø±Ø¯ Ø­Ø³Ø§Ø¨ LinkedIn Ø®ÙˆØ¯ Ø´ÙˆÛŒØ¯ Ùˆ Ú©Ù¾Ú†Ø§ Ø±Ø§ Ø­Ù„ Ú©Ù†ÛŒØ¯.")
            st.code("â³ Ù…Ù†ØªØ¸Ø± Ø¨Ù…Ø§Ù†ÛŒØ¯ ØªØ§ Ø¨Ø¹Ø¯ Ø§Ø² ÙˆØ±ÙˆØ¯ Ø¨Ù‡ ØµÙØ­Ù‡ Ø§ØµÙ„ÛŒ LinkedIn Ø¨Ø±ÙˆÛŒØ¯...")

        scraper.login()

        with st.spinner("ğŸ” Ø¯Ø± Ø­Ø§Ù„ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø´ØºÙ„..."):
            scraper.jobs(job_title, location)

        with st.spinner("ğŸ“¥ Ø¯Ø± Ø­Ø§Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª..."):
            scraper.job_scraper(count)

        st.success("âœ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯!")

    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ÙØ±Ø¢ÛŒÙ†Ø¯ Ø§Ø³ØªØ®Ø±Ø§Ø¬: {e}")

    finally:
        try:
            scraper.driver.quit()
        except:
            pass

if os.path.exists(CSV_PATH):
    st.subheader("ğŸ“‹ Ù†ØªØ§ÛŒØ¬ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡:")
    df = pd.read_csv(CSV_PATH)

    # ===========================
    # ğŸ›ï¸ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ ØªØ¹Ø§Ù…Ù„ÛŒ
    # ===========================
    with st.expander("ğŸ”§ ÙÛŒÙ„ØªØ± Ù†ØªØ§ÛŒØ¬"):
        location_filter = st.multiselect("ğŸŒ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÙˆÙ‚Ø¹ÛŒØª Ù…Ú©Ø§Ù†ÛŒ", df["Location"].dropna().unique())
        company_filter = st.multiselect("ğŸ¢ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø±Ú©Øª", df["Company"].dropna().unique())
        title_search = st.text_input("ğŸ” Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø¹Ù†ÙˆØ§Ù† Ø´ØºÙ„ÛŒ")

    filtered_df = df.copy()
    if location_filter:
        filtered_df = filtered_df[filtered_df["Location"].isin(location_filter)]
    if company_filter:
        filtered_df = filtered_df[filtered_df["Company"].isin(company_filter)]
    if title_search:
        filtered_df = filtered_df[filtered_df["Title"].str.contains(title_search, case=False, na=False)]

    st.metric("ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ù…Ø´Ø§ØºÙ„ Ø¨Ø¹Ø¯ Ø§Ø² ÙÛŒÙ„ØªØ±", len(filtered_df))

    # ===========================
    # ğŸ“ˆ Ù†Ù…ÙˆØ¯Ø§Ø± Ø¢Ù…Ø§Ø±ÛŒ Ø³Ø§Ø¯Ù‡
    # ===========================
    with st.expander("ğŸ“Š Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ø¢Ù…Ø§Ø±ÛŒ"):
        location_count = filtered_df['Location'].value_counts().head(10)
        st.bar_chart(location_count)

    # ===========================
    # ğŸ“‹ Ø¬Ø¯ÙˆÙ„ Ù†ØªØ§ÛŒØ¬ Ø¨Ø§ Ù„ÛŒÙ†Ú©
    # ===========================
    def make_clickable(link):
        return f'<a href="{link}" target="_blank">Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø¯Ø± Ù„ÛŒÙ†Ú©Ø¯ÛŒÙ†</a>'

    filtered_df['Ù„ÛŒÙ†Ú©'] = filtered_df['Link'].apply(make_clickable)
    st.write("### ğŸ“„ Ù†ØªØ§ÛŒØ¬ Ø´ØºÙ„ÛŒ")
    st.write(filtered_df[['Title', 'Company', 'Location', 'Salary', 'Ù„ÛŒÙ†Ú©']].to_html(escape=False, index=False), unsafe_allow_html=True)

    # ===========================
    # ğŸ“¤ Ø¯Ú©Ù…Ù‡ Ø¯Ø§Ù†Ù„ÙˆØ¯
    # ===========================
    st.download_button(
        label="â¬‡ï¸ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù†ØªØ§ÛŒØ¬ ÙÛŒÙ„ØªØ± Ø´Ø¯Ù‡",
        data=filtered_df.to_csv(index=False),
        file_name="filtered_jobs.csv",
        mime="text/csv"
    )
