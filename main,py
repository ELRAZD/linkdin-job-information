import time
import csv
import re
import os
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

class Data_Mining():
    def __init__(self):
        self.path = r"C:\Users\Elyar\Desktop\New folder"  # Ù…Ø³ÛŒØ± Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„ Ùˆ chromedriver
        self.driver_path = os.path.join(self.path, "chromedriver.exe")
        self.driver = None

    def setup_driver(self):
        if not os.path.exists(self.driver_path):
            raise FileNotFoundError(f"chromedriver Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ Ø¯Ø±: {self.driver_path}")

        chrome_options = Options()
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_argument("--disable-infobars")
        chrome_options.add_argument("--disable-extensions")

        self.driver = webdriver.Chrome(service=Service(self.driver_path), options=chrome_options)

    def login(self):
        self.driver.get("https://www.linkedin.com/login/")
        print("ğŸ”‘ Ù„Ø·ÙØ§Ù‹ Ø§ÛŒÙ…ÛŒÙ„ Ùˆ Ø±Ù…Ø² Ø¹Ø¨ÙˆØ± Ø®ÙˆØ¯ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯ Ùˆ Ù„Ø§Ú¯ÛŒÙ† Ø´ÙˆÛŒØ¯ (Ùˆ Ú©Ù¾Ú†Ø§ Ø±Ø§ Ø­Ù„ Ú©Ù†ÛŒØ¯ Ø§Ú¯Ø± Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯)...")

        try:
            WebDriverWait(self.driver, 180).until(
                EC.presence_of_element_located((By.ID, "global-nav-search"))
            )
            print("âœ… ÙˆØ±ÙˆØ¯ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")
        except:
            print("â›” Ø²Ù…Ø§Ù† ÙˆØ±ÙˆØ¯ ØªÙ…Ø§Ù… Ø´Ø¯ ÛŒØ§ ÙˆØ±ÙˆØ¯ Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯.")
            self.driver.quit()
            exit()

    def jobs(self, job_name: str, location: str):
        self.driver.get("https://www.linkedin.com/jobs/")
        time.sleep(5)

        job_name_input = self.driver.find_element(By.CSS_SELECTOR, 'input[aria-label="Search by title, skill, or company"]')
        job_name_input.clear()
        job_name_input.send_keys(job_name)
        time.sleep(1)

        location_input = self.driver.find_element(By.CSS_SELECTOR, 'input[aria-label="City, state, or zip code"]')
        location_input.clear()
        location_input.send_keys(location)
        time.sleep(1)
        location_input.send_keys(Keys.RETURN)
        time.sleep(5)

    def job_scraper(self, counts: int):
        def loop_count(counts: int):
            return (counts // 25) + (1 if counts % 25 != 0 else 0)

        def clean_salary(text: str):
            match = re.search(r"\$[\d.,K]+/?[a-zA-Z]*(?:\s*-\s*\$[\d.,K]+/?[a-zA-Z]*)?", text)
            return match.group() if match else "None"

        job_data = []

        for _ in range(loop_count(counts)):
            time.sleep(2)
            jobs = self.driver.find_elements(By.XPATH, "//li[@data-occludable-job-id]")

            for job in jobs:
                self.driver.execute_script("arguments[0].scrollIntoView();", job)
                try:
                    job_id = job.get_attribute("data-occludable-job-id")

                    job_title_element = job.find_element(By.XPATH, ".//a[contains(@class, 'job-card-container__link')]")
                    job_title = job_title_element.text.strip()

                    company_element = job.find_element(By.XPATH, ".//div[contains(@class, 'artdeco-entity-lockup__subtitle')]")
                    company_name = company_element.text.strip()

                    location = "None"
                    try:
                        location_element = job.find_element(By.XPATH, ".//div[contains(@class, 'artdeco-entity-lockup__caption')]//li")
                        location = location_element.text.strip()
                    except:
                        pass

                    salary = "None"
                    try:
                        salary_element = job.find_element(By.XPATH, ".//div[contains(@class, 'artdeco-entity-lockup__metadata')]//li")
                        salary = clean_salary(salary_element.text.strip())
                    except:
                        pass

                    job_data.append({
                        'ID': job_id,
                        'Link': f"https://www.linkedin.com/jobs/view/{job_id}/",
                        'Title': job_title,
                        'Company': company_name,
                        'Location': location,
                        'Salary': salary
                    })
                except Exception as e:
                    print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø´ØºÙ„: {e}")
                    continue

            # Ø±ÙØªÙ† Ø¨Ù‡ ØµÙØ­Ù‡ Ø¨Ø¹Ø¯
            try:
                next_button_element = self.driver.find_element(By.XPATH, "//button[@aria-label='View next page']")
                if next_button_element.is_enabled():
                    next_button_element.click()
                else:
                    print("ğŸ”š Ø¯Ú©Ù…Ù‡ Ø¨Ø¹Ø¯ÛŒ ØºÛŒØ±ÙØ¹Ø§Ù„ Ø§Ø³Øª. ØªÙˆÙ‚Ù.")
                    break
            except Exception as e:
                print(f"â›” Ù†ØªÙˆØ§Ù†Ø³Øª Ø¯Ú©Ù…Ù‡ Ø¨Ø¹Ø¯ÛŒ Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ú©Ù†Ø¯ ÛŒØ§ Ú©Ù„ÛŒÚ© Ú©Ù†Ø¯: {e}")
                break

        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± CSV
        job_data_path = os.path.join(self.path, "job_data.csv")
        with open(job_data_path, "w", newline='', encoding='utf-8') as jobs_data:
            writer = csv.DictWriter(jobs_data, fieldnames=["ID", "Link", "Title", "Company", "Location", "Salary"])
            writer.writeheader()
            writer.writerows(job_data)

        print(f"âœ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(job_data)} Ø´ØºÙ„ Ú©Ø§Ù…Ù„ Ø´Ø¯. ÙØ§ÛŒÙ„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯ Ø¯Ø±: {job_data_path}")


# ğŸ§ª Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡
if __name__ == "__main__":
    data = Data_Mining()
    data.setup_driver()
    data.login()  # ÙˆØ±ÙˆØ¯ Ø¯Ø³ØªÛŒ ØªÙˆØ³Ø· Ú©Ø§Ø±Ø¨Ø±
    data.jobs('java', 'germany')  # ğŸ” Ø¬Ø³ØªØ¬ÙˆÛŒ Ø´ØºÙ„
    data.job_scraper(50) 